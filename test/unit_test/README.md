# RAGFlow Unit Test Suite

Comprehensive unit tests for RAGFlow core services and features.

## ðŸ“ Test Structure

```
test/unit_test/
â”œâ”€â”€ common/                    # Utility function tests
â”‚   â”œâ”€â”€ test_decorator.py
â”‚   â”œâ”€â”€ test_file_utils.py
â”‚   â”œâ”€â”€ test_float_utils.py
â”‚   â”œâ”€â”€ test_misc_utils.py
â”‚   â”œâ”€â”€ test_string_utils.py
â”‚   â”œâ”€â”€ test_time_utils.py
â”‚   â””â”€â”€ test_token_utils.py
â”œâ”€â”€ services/                  # Service layer tests (NEW)
â”‚   â”œâ”€â”€ test_dialog_service.py
â”‚   â”œâ”€â”€ test_conversation_service.py
â”‚   â”œâ”€â”€ test_canvas_service.py
â”‚   â”œâ”€â”€ test_knowledgebase_service.py
â”‚   â””â”€â”€ test_document_service.py
â””â”€â”€ README.md                  # This file
```

## ðŸ§ª Test Coverage

### Dialog Service Tests (`test_dialog_service.py`)
- âœ… Dialog creation, update, deletion
- âœ… Dialog retrieval by ID and tenant
- âœ… Name validation (empty, length limits)
- âœ… LLM settings validation
- âœ… Prompt configuration validation
- âœ… Knowledge base linking
- âœ… Duplicate name handling
- âœ… Pagination and search
- âœ… Status management
- **Total: 30+ test cases**

### Conversation Service Tests (`test_conversation_service.py`)
- âœ… Conversation creation with prologue
- âœ… Message management (add, delete, update)
- âœ… Reference handling with chunks
- âœ… Thumbup/thumbdown feedback
- âœ… Message structure validation
- âœ… Conversation ordering
- âœ… Batch operations
- âœ… Audio binary support
- **Total: 35+ test cases**

### Canvas/Agent Service Tests (`test_canvas_service.py`)
- âœ… Canvas creation, update, deletion
- âœ… DSL structure validation
- âœ… Component and edge validation
- âœ… Permission management (me/team)
- âœ… Canvas categories (agent/dataflow)
- âœ… Async execution testing
- âœ… Debug mode testing
- âœ… Version management
- âœ… Complex workflow testing
- **Total: 40+ test cases**

### Knowledge Base Service Tests (`test_knowledgebase_service.py`)
- âœ… KB creation, update, deletion
- âœ… Name validation
- âœ… Embedding model validation
- âœ… Parser configuration
- âœ… Language support
- âœ… Document/chunk/token statistics
- âœ… Batch operations
- âœ… Embedding model consistency
- **Total: 35+ test cases**

### Document Service Tests (`test_document_service.py`)
- âœ… Document upload and management
- âœ… File type validation
- âœ… Size validation
- âœ… Parsing status progression
- âœ… Progress tracking
- âœ… Chunk and token counting
- âœ… Batch upload/delete
- âœ… Search and pagination
- âœ… Parser configuration
- **Total: 35+ test cases**

## ðŸš€ Running Tests

### Run All Unit Tests
```bash
cd /root/74/ragflow
pytest test/unit_test/ -v
```

### Run Specific Test File
```bash
pytest test/unit_test/services/test_dialog_service.py -v
```

### Run Specific Test Class
```bash
pytest test/unit_test/services/test_dialog_service.py::TestDialogService -v
```

### Run Specific Test Method
```bash
pytest test/unit_test/services/test_dialog_service.py::TestDialogService::test_dialog_creation_success -v
```

### Run with Coverage Report
```bash
pytest test/unit_test/ --cov=api/db/services --cov-report=html
```

### Run Tests in Parallel
```bash
pytest test/unit_test/ -n auto
```

## ðŸ“Š Test Markers

Tests use pytest markers for categorization:

- `@pytest.mark.unit` - Unit tests (isolated, mocked)
- `@pytest.mark.integration` - Integration tests (with database)
- `@pytest.mark.asyncio` - Async tests
- `@pytest.mark.parametrize` - Parameterized tests

## ðŸ› ï¸ Test Fixtures

### Common Fixtures

**`mock_dialog_service`** - Mocked DialogService for testing
```python
@pytest.fixture
def mock_dialog_service(self):
    with patch('api.db.services.dialog_service.DialogService') as mock:
        yield mock
```

**`sample_dialog_data`** - Sample dialog data
```python
@pytest.fixture
def sample_dialog_data(self):
    return {
        "id": get_uuid(),
        "tenant_id": "test_tenant_123",
        "name": "Test Dialog",
        ...
    }
```

## ðŸ“ Writing New Tests

### Test Class Template

```python
import pytest
from unittest.mock import Mock, patch
from common.misc_utils import get_uuid

class TestYourService:
    """Comprehensive unit tests for YourService"""

    @pytest.fixture
    def mock_service(self):
        """Create a mock service for testing"""
        with patch('api.db.services.your_service.YourService') as mock:
            yield mock

    @pytest.fixture
    def sample_data(self):
        """Sample data for testing"""
        return {
            "id": get_uuid(),
            "name": "Test Item",
            ...
        }

    def test_creation_success(self, mock_service, sample_data):
        """Test successful creation"""
        mock_service.save.return_value = True
        result = mock_service.save(**sample_data)
        assert result is True

    def test_validation_error(self):
        """Test validation error handling"""
        with pytest.raises(Exception):
            if not valid_condition:
                raise Exception("Validation failed")
```

### Parameterized Test Template

```python
@pytest.mark.parametrize("input_value,expected", [
    ("valid", True),
    ("invalid", False),
    ("", False),
])
def test_validation(self, input_value, expected):
    """Test validation with different inputs"""
    result = validate(input_value)
    assert result == expected
```

## ðŸ” Test Best Practices

1. **Isolation**: Each test should be independent
2. **Mocking**: Use mocks for external dependencies
3. **Clarity**: Test names should describe what they test
4. **Coverage**: Aim for >80% code coverage
5. **Speed**: Unit tests should run quickly (<1s each)
6. **Assertions**: Use specific assertions with clear messages

## ðŸ“ˆ Test Metrics

Current test suite statistics:

- **Total Test Files**: 5 (services) + 7 (common) = 12
- **Total Test Cases**: 175+
- **Test Coverage**: Services layer
- **Execution Time**: ~5-10 seconds

## ðŸ› Debugging Tests

### Run with Verbose Output
```bash
pytest test/unit_test/ -vv
```

### Run with Print Statements
```bash
pytest test/unit_test/ -s
```

### Run with Debugging
```bash
pytest test/unit_test/ --pdb
```

### Run Failed Tests Only
```bash
pytest test/unit_test/ --lf
```

## ðŸ“š Dependencies

Required packages for testing:
```
pytest>=7.0.0
pytest-asyncio>=0.21.0
pytest-cov>=4.0.0
pytest-mock>=3.10.0
pytest-xdist>=3.0.0  # For parallel execution
```

Install with:
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock pytest-xdist
```

## ðŸŽ¯ Future Enhancements

- [ ] Integration tests with real database
- [ ] API endpoint tests
- [ ] Performance/load tests
- [ ] Frontend component tests
- [ ] End-to-end tests
- [ ] Continuous integration setup
- [ ] Test coverage badges
- [ ] Mutation testing

## ðŸ“ž Support

For questions or issues with tests:
1. Check test output for error messages
2. Review test documentation
3. Check existing test examples
4. Open an issue on GitHub

## ðŸ“„ License

Copyright 2025 The InfiniFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0.
